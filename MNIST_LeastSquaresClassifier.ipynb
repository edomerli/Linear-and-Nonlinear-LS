{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Least Squares Based Supervised Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Proof of correctness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\def\\im{\\implies}\n",
    "\\def\\R{{\\rm I\\!R}}\n",
    "$\n",
    "\n",
    "\\begin{align}\n",
    "A^Hy = A^HA\\hat{x}&\\im A^Hy - A^HA\\hat{x} = 0 \\\\\n",
    "    &\\im A^H(y - A\\hat{x}) = 0 \\\\\n",
    "    &\\im a_i^H(y - A\\hat{x}) = 0 \\quad \\forall i=1, ..., n \\\\\n",
    "    &\\im \\langle y - A\\hat{x}, a_i\\rangle = 0 \\quad \\forall i = 1, ..., n \\\\\n",
    "    &\\im y - A\\hat{x} \\ \\bot \\ a_i \\quad \\forall i = 1, ..., n \\\\\n",
    "    &\\im y - A\\hat{x} \\ \\bot \\ span\\{a_1, ..., a_n\\} = R(A) \\\\\n",
    "    &\\im A\\hat{x} \\textit{ is the projection of y into R(A)} \\\\\n",
    "    &\\im \\hat{x} \\textit{ is the solution to } \\min_{x} \\|y-Ax\\|_2^2 &&\\textit{(Proven in class)} \\\\\n",
    "    &\\im \\hat{x} \\textit{ is the solution to } \\min_x \\|y-Ax\\|_2 &&\\textit{(Since } \\sqrt{\\cdot} \\textit{ is a strictly decreasing function)}\\\\\n",
    "\\end{align}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Will prove that $ A^Hy = A^HAx $ has always a solution by proving that $R(A^H) = R(A^HA)$, hence:\n",
    "\n",
    "$$\n",
    "    \\forall y \\ \\exists x \\ s.t. A^Hy = A^HAx\n",
    "$$\n",
    "\n",
    "Let $ \\{b_1, ..., b_k\\} $ be a basis of $R(A^HA)$\n",
    "\n",
    "\\begin{align}\n",
    "R(A) \\subseteq \\R^m &\\im R(A^HA) = \\{A^Hz \\mid z \\in R(A)\\} \\ \\subseteq \\  \\{A^Hy \\mid y \\in \\R^m\\} = R(A^H) \\\\\n",
    "    &\\im b_1, ..., b_k \\in R(A^H) \\\\\n",
    "    &\\im b_1, ..., b_k \\ \\textit{linearly independent vectors in R(A^H)} &&(dim(R(A^H)) = dim(R(A^HA)) = k)\\\\\n",
    "    &\\im b_1, ..., b_k \\ \\textit{basis of} \\ R(A^H) \\\\\n",
    "    &\\im R(A^HA) = span\\{b_1, ..., b_k\\} = R(A^H)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b)\n",
    "\n",
    "Case 1: $rank(A) = n \\im rank(A^HA) = n \\im A^Hy = A^HAx \\ \\text{has 1 unique solution}$\n",
    "\n",
    "Case 2: $rank(A) < n \\im rank(A^HA) < n \\im A^Hy = A^HAx \\ \\text{has} \\ \\infty \\ \\text{solutions} \\\\\n",
    "        (\\text{because since } N(A^HA) \\neq \\{0\\}, \\ A^Hy = A^HAx \\im \\forall \\alpha \\in \\R. \\ A^Hy = A^HA(x + \\alpha z) \\text{ with } z \\in N(A))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Squares Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "LS regression model\n",
    "    :param A: matrix in R^{mxn}\n",
    "    :param y: vector in R^m\n",
    "    :return: x that minimizes (L2-Norm(y - A*x))^2\n",
    "'''\n",
    "def LS_regression(A, y):\n",
    "    square_A = A.conj().T @ A\n",
    "    u, s, vh = np.linalg.svd(square_A)\n",
    "    s = s[s >= 1.0e-3]\n",
    "    r = s.size\n",
    "    u = u[:, :r]\n",
    "    vh = vh[:r]\n",
    "    x = (vh.conj().T * np.reciprocal(s)) @ u.conj().T @ A.conj().T @ y\n",
    "    return np.squeeze(x)\n",
    "\n",
    "def sign(n):\n",
    "    if n >= 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def ones_padding(X):\n",
    "    m, n = X.shape\n",
    "    ones = np.ones((m, 1))\n",
    "    X_padded = np.hstack((X, ones))\n",
    "    return X_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Binary classifier\n",
    "    :param X: matrix in R^{Nxm}, every row is the feature vector x^(i) in R^m\n",
    "    :param y: vector in R^m, every entry is the label y^(i) for data point i, y^(i) in {-1, 1}\n",
    "    :param pad: wether to pad X with a 1s column or not, default is True\n",
    "    :return: f_hat, function that using (beta, alpha) that minimize (L2-Norm(y - (X|1) * (beta,alpha)))^2\n",
    "             predicts a class in {-1, 1} for a given input x\n",
    "'''\n",
    "def binary_classifier(X, y, pad=True):\n",
    "    X_one = ones_padding(X) if pad else np.copy(X)\n",
    "    beta_alpha = LS_regression(X_one, y)\n",
    "    \n",
    "    def f_hat(x):\n",
    "        if pad:\n",
    "            return sign(beta_alpha[:-1].T @ x - beta_alpha[-1])\n",
    "        else:\n",
    "            return sign(beta_alpha.T @ x)\n",
    "    \n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multi-Class One-versus-All classifier\n",
    "    :param X: matrix in R^{Nxm}, every row is the feature vector x^(i) in R^m\n",
    "    :param y: vector in R^m, every entry is the label y^(i) for data point i, y^(i) in {0, ..., k-1}\n",
    "    :param k: the number of classes\n",
    "    :param pad: wether to pad X with a 1s column or not, default is True\n",
    "    :return: f_hat, function that using (beta, alpha) that minimize (L2-Norm(y - (X|1) * (beta,alpha)))^2\n",
    "             predicts a class in {0, ..., k-1} for a given input x\n",
    "'''\n",
    "def one_vs_all_classifier(X, y, k, pad=True):\n",
    "    N, m = X.shape\n",
    "    X_one = ones_padding(X) if pad else np.copy(X)\n",
    "    Beta_Alpha = np.zeros((k, m+(1 if pad else 0)))\n",
    "    \n",
    "    for i in range(k):\n",
    "        y_k = np.where(y == i, 1, -1)\n",
    "        Beta_Alpha[i] = LS_regression(X_one, y_k)\n",
    "    \n",
    "    def f_hat(x):\n",
    "        if pad:\n",
    "            return np.argmax(Beta_Alpha[:, :-1] @ x + Beta_Alpha[:, -1])\n",
    "        else:\n",
    "            return np.argmax(Beta_Alpha @ x)\n",
    "    \n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Multi-Class One-versus-One classifier\n",
    "    :param X: matrix in R^{Nxm}, every row is the feature vector x^(i) in R^m\n",
    "    :param y: vector in R^m, every entry is the label y^(i) for data point i, y^(i) in {0, ..., k-1}\n",
    "    :param k: the number of classes\n",
    "    :return: f_hat, function that using (beta, alpha) that minimize (L2-Norm(y - (X|1) * (beta,alpha)))^2\n",
    "             predicts a class in {0, ..., k-1} for a given input x\n",
    "'''\n",
    "def one_vs_one_classifier(X, y, k, pad=True):\n",
    "    Bin_classifiers = []\n",
    "    \n",
    "    for i in range(k):\n",
    "        for j in range(i+1, k):\n",
    "            filt = np.squeeze(np.logical_or(y == i, y == j))\n",
    "            X_ij = X[filt]\n",
    "            y_ij = y[filt]\n",
    "            y_ij = np.where(y_ij == i, -1, 1)\n",
    "            Bin_classifiers.append(binary_classifier(X_ij, y_ij, pad))\n",
    "    \n",
    "    def f_hat(x):\n",
    "        votes = np.zeros(k)\n",
    "        z = 0\n",
    "        for i in range(k):\n",
    "            for j in range(i+1, k):\n",
    "                vote = Bin_classifiers[z](x)\n",
    "                votes[i if vote == -1 else j] += 1\n",
    "                z += 1\n",
    "        return np.argmax(votes)\n",
    "    \n",
    "    return f_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "\n",
    "mnist = sio.loadmat('mnist.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = (mnist['testX'].astype('float')) / 255.0\n",
    "testY = mnist['testY'].T.astype('float')\n",
    "trainX = (mnist['trainX'].astype('float')) / 255.0\n",
    "trainY = mnist['trainY'].T.astype('float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784) (10000, 1) (60000, 784) (60000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(testX.shape, testY.shape, trainX.shape, trainY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_hat_onevall = one_vs_all_classifier(trainX, trainY, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_hat_onevone = one_vs_one_classifier(trainX, trainY, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(f_hat, data, labels, k):\n",
    "    results = np.apply_along_axis(f_hat, 1, data)\n",
    "    l = np.squeeze(labels)\n",
    "    print('Error rate: ', np.sum(results != l)/data.shape[0])\n",
    "    \n",
    "    conf_matrix = np.zeros((k, k), dtype=int)\n",
    "    for i in range(results.shape[0]):\n",
    "        conf_matrix[int(l[i]), int(results[i])] += 1\n",
    "        \n",
    "    print('Confusion matrix: \\n', conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus All classifier performance on training data: \n",
      "\n",
      "Error rate:  0.14228333333333334\n",
      "Confusion matrix: \n",
      " [[5682    7   18   14   24   43   64    4   61    6]\n",
      " [   2 6548   40   15   19   31   14   12   55    6]\n",
      " [  99  264 4792  149  108   11  234   91  192   18]\n",
      " [  42  167  176 5158   32  125   56  115  135  125]\n",
      " [  10   99   42    6 5212   50   39   23   59  302]\n",
      " [ 164   95   28  431  105 3992  192   36  235  143]\n",
      " [ 108   74   61    1   70   90 5476    0   35    3]\n",
      " [  55  189   37   47  170    9    2 5426   10  320]\n",
      " [  75  494   63  226  105  222   56   20 4410  180]\n",
      " [  68   60   20  117  371   12    4  492   38 4767]]\n"
     ]
    }
   ],
   "source": [
    "print('One versus All classifier performance on training data: \\n')\n",
    "evaluate(f_hat_onevall, trainX, trainY, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus One classifier performance on training data: \n",
      "\n",
      "Error rate:  0.2252\n",
      "Confusion matrix: \n",
      " [[5551    7    5    9    5   10   41    0  295    0]\n",
      " [   0 6659    4    8    1    0    2    0   64    4]\n",
      " [  19  406 4166  237   41    0  159    6  908   16]\n",
      " [  15  446   42 4359    5    1   32    4 1204   23]\n",
      " [  14  183   18    2 4259    0  113    1  125 1127]\n",
      " [  67  236   21  359   55  217  230    0 4150   86]\n",
      " [  15   55   12    1    4    1 5702    0  128    0]\n",
      " [  24  338   40   93   85    0   10 4618  318  739]\n",
      " [   5  230    1    9    2    0   36    0 5564    4]\n",
      " [  15   84   16   33   32    0    6   15  355 5393]]\n"
     ]
    }
   ],
   "source": [
    "print('One versus One classifier performance on training data: \\n')\n",
    "evaluate(f_hat_onevone, trainX, trainY, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus All classifier performance on test data: \n",
      "\n",
      "Error rate:  0.1398\n",
      "Confusion matrix: \n",
      " [[ 944    0    1    2    2    7   14    2    7    1]\n",
      " [   0 1107    2    2    3    1    5    1   14    0]\n",
      " [  18   54  813   26   15    0   42   22   37    5]\n",
      " [   4   18   23  879    5   17    9   21   22   12]\n",
      " [   0   22    6    1  881    5   10    2   11   44]\n",
      " [  23   18    3   72   24  659   23   14   39   17]\n",
      " [  18   10    9    0   22   17  875    0    7    0]\n",
      " [   5   40   16    6   26    0    1  884    0   50]\n",
      " [  14   46   11   30   27   40   15   12  759   20]\n",
      " [  15   11    2   17   80    1    1   77    4  801]]\n"
     ]
    }
   ],
   "source": [
    "print('One versus All classifier performance on test data: \\n')\n",
    "evaluate(f_hat_onevall, testX, testY, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus One classifier performance on test data: \n",
      "\n",
      "Error rate:  0.2315\n",
      "Confusion matrix: \n",
      " [[ 932    1    1    2    1    1   11    0   31    0]\n",
      " [   0 1123    1    3    0    0    1    0    7    0]\n",
      " [   8   87  680   41    8    0   25    1  181    1]\n",
      " [   3   63    6  715    0    0    4    0  214    5]\n",
      " [   2   34    6    1  703    0   26    0   24  186]\n",
      " [  10   32    2   62    6   40   31    0  695   14]\n",
      " [   8    9    5    0    3    0  912    0   21    0]\n",
      " [   3   63   10   15   15    0    2  744   64  112]\n",
      " [   4   24    1    4    4    0   10    0  926    1]\n",
      " [   6   14    0    6    4    0    1    0   68  910]]\n"
     ]
    }
   ],
   "source": [
    "print('One versus One classifier performance on test data: \\n')\n",
    "evaluate(f_hat_onevone, testX, testY, K)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models generalized well on the test data: the error rate doesn't vary too much between training and test, hence the models didn't overfit to the training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the test data, the digits that are:\n",
    "* easier to classify are 1s and 6s, since they have the lowest percentage of \"false negatives\" (1s/6s that are classified as not 1s/6s)\n",
    "* harder to classify are 5s by far, with the lowest percentage of true positives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Randomized Feature Based LS Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (a) Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 1000\n",
    "Wb = np.random.normal(0, 1, size=(L, 784+1))\n",
    "\n",
    "\"\"\"\n",
    "Randomoized Feature Based LS CLassifier\n",
    "    :param X: the training data\n",
    "    :param y: the training labels\n",
    "    :param L: dimension of the \"feature space\"\n",
    "    :param g: real valued function to compute the feature mappings\n",
    "    :param classifier_type: either \"one_vs_one\" or \"one_vs_all\"\n",
    "    :param K: number of classes\n",
    "    :return: f_hat the respective classifier trained on the feature mappings \n",
    "\"\"\"\n",
    "def randomized_features_classifier(X, y, Wb, g, classifier_type, k):\n",
    "    N, m = X.shape\n",
    "    X_one = ones_padding(X)\n",
    "    h_X = g(X_one @ Wb.T)\n",
    "    if classifier_type == 'one_vs_one':\n",
    "        return one_vs_one_classifier(h_X, y, K, pad=False)\n",
    "    elif classifier_type == 'one_vs_all':\n",
    "        return one_vs_all_classifier(h_X, y, K, pad=False)\n",
    "    else:\n",
    "        raise ValueError(\"classifier type not valid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (b) Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def g1_identity(x):\n",
    "    return x\n",
    "\n",
    "def g2_sigmoid(x):\n",
    "    return 1/(1 + np.exp(-x))\n",
    "\n",
    "def g3_sinusoidal(x):\n",
    "    return np.sin(x)\n",
    "\n",
    "def g4_ReLU(x):\n",
    "    return np.maximum(x, 0)\n",
    "\n",
    "g = [g1_identity, g2_sigmoid, g3_sinusoidal, g4_ReLU]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One vs All Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ovall = []\n",
    "for i in range(len(g)):\n",
    "    f_ovall.append(randomized_features_classifier(trainX, trainY, Wb, g[i], 'one_vs_all', K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One vs One Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_ovone = []\n",
    "for i in range(len(g)):\n",
    "    f_ovone.append(randomized_features_classifier(trainX, trainY, Wb, g[i], 'one_vs_one', K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (c) Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_feature(f_hat, g, Wb, data, labels, k):\n",
    "    N, m = data.shape\n",
    "    data_one = ones_padding(data)\n",
    "    h_data = g(data_one @ Wb.T)\n",
    "    evaluate(f_hat, h_data, labels, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus All randomized feature classifier performance: \n",
      "\n",
      "\n",
      "With g_1(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.14226666666666668\n",
      "Confusion matrix: \n",
      " [[5682    7   18   14   24   43   64    4   61    6]\n",
      " [   2 6548   40   15   19   31   14   12   55    6]\n",
      " [  99  264 4792  149  108   11  234   91  192   18]\n",
      " [  42  167  176 5158   32  125   56  115  135  125]\n",
      " [  10   99   42    6 5212   50   39   23   59  302]\n",
      " [ 164   95   28  432  105 3991  192   36  235  143]\n",
      " [ 108   74   61    1   70   90 5476    0   35    3]\n",
      " [  55  189   37   47  170    9    2 5426   10  320]\n",
      " [  75  493   63  226  105  221   56   20 4412  180]\n",
      " [  68   60   20  117  371   12    4  492   38 4767]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.1397\n",
      "Confusion matrix: \n",
      " [[ 944    0    1    2    2    7   14    2    7    1]\n",
      " [   0 1107    2    2    3    1    5    1   14    0]\n",
      " [  18   54  813   26   15    0   42   22   37    5]\n",
      " [   4   17   23  880    5   17    9   21   22   12]\n",
      " [   0   22    6    1  881    5   10    2   11   44]\n",
      " [  23   18    3   72   24  659   23   14   39   17]\n",
      " [  18   10    9    0   22   17  875    0    7    0]\n",
      " [   5   40   16    6   26    0    1  884    0   50]\n",
      " [  14   46   11   30   27   40   15   12  759   20]\n",
      " [  15   11    2   17   80    1    1   77    4  801]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_2(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.06938333333333334\n",
      "Confusion matrix: \n",
      " [[5794    0    6    9   12   15   36    7   41    3]\n",
      " [   2 6605   45   13   15   18   12    9   17    6]\n",
      " [  54   28 5445   68   79   11   59   79  123   12]\n",
      " [  29   17  131 5515   15  146   26   66  119   67]\n",
      " [   7   32   21    4 5504   13   49   12   30  170]\n",
      " [  67   25   26  153   36 4896   94   27   57   40]\n",
      " [  38   12   28    4   22   93 5701    0   19    1]\n",
      " [  27   63   54   30   79   10    5 5833   15  149]\n",
      " [  24   60   73  142   47  140   41   24 5213   87]\n",
      " [  39   29   23   72  178   32    5  173   67 5331]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.0688\n",
      "Confusion matrix: \n",
      " [[ 966    1    1    0    0    2    7    2    1    0]\n",
      " [   0 1119    2    4    1    0    5    0    4    0]\n",
      " [  10    0  937   13    7    3   12   12   34    4]\n",
      " [   5    1   14  924    2   18    2   14   19   11]\n",
      " [   1    3    7    3  911    2    7    1    9   38]\n",
      " [  11    1    2   32    5  800   16    9   11    5]\n",
      " [  12    4    3    1    6   11  918    0    3    0]\n",
      " [   2   14   19    4   12    0    0  951    2   24]\n",
      " [   8    1   11   18    9   23   10   10  876    8]\n",
      " [   8    9    2   13   27    5    3   24    8  910]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_3(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.7981833333333334\n",
      "Confusion matrix: \n",
      " [[ 894 1725  411  398  467  398  397  471  403  359]\n",
      " [ 269 4489  280  266  262  207  232  256  255  226]\n",
      " [ 414 1656  912  453  478  433  384  474  394  360]\n",
      " [ 478 1756  471  829  430  420  415  477  449  406]\n",
      " [ 443 1604  439  435  886  379  426  459  394  377]\n",
      " [ 412 1553  439  365  401  724  385  392  390  360]\n",
      " [ 424 1688  406  409  410  417  849  485  438  392]\n",
      " [ 452 1780  477  433  467  438  424  965  438  391]\n",
      " [ 401 1720  436  412  447  365  421  468  780  401]\n",
      " [ 459 1694  439  410  472  373  421  463  437  781]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.85\n",
      "Confusion matrix: \n",
      " [[ 89 270  86  78  85  61  76  88  61  86]\n",
      " [ 42 742  55  40  44  39  49  45  34  45]\n",
      " [ 75 324  87  82  72  64  93  83  75  77]\n",
      " [ 87 286  91  94  65  58  82  88  73  86]\n",
      " [ 79 270  87  61  96 101  77  72  65  74]\n",
      " [ 74 246  72  68  86  73  59  75  66  73]\n",
      " [ 76 289  77  88  82  68  80  80  60  58]\n",
      " [ 86 306  63  79 101  86  72  97  70  68]\n",
      " [ 95 271  75  71  97  64  73  69  73  86]\n",
      " [ 85 282  90  83  82  77  62  83  96  69]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_4(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.05515\n",
      "Confusion matrix: \n",
      " [[5795    1    8    4    8   17   39    8   39    4]\n",
      " [   1 6623   34    9   13   15   13   11   20    3]\n",
      " [  32   31 5558   58   51   13   31   80   91   13]\n",
      " [  16   19   79 5656    8  117   18   55   98   65]\n",
      " [  11   31   16    7 5519    9   43    9   31  166]\n",
      " [  28   20   12   94   25 5071   82   16   37   36]\n",
      " [  31    9   17    0   21   72 5745    0   20    3]\n",
      " [  18   62   45   23   60    4    3 5872   16  162]\n",
      " [  13   60   42   96   36   81   40   12 5387   84]\n",
      " [  25   25   11   75  135   30    2  135   46 5465]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.0577\n",
      "Confusion matrix: \n",
      " [[ 961    1    0    0    1    3   10    1    3    0]\n",
      " [   0 1121    2    2    1    0    4    0    5    0]\n",
      " [   7    3  944   12    4    0    7   13   37    5]\n",
      " [   1    3   11  938    2   23    1   12   15    4]\n",
      " [   2    2    4    0  920    2    7    3    8   34]\n",
      " [   8    1    2   12    5  829   12    7   11    5]\n",
      " [  11    3    1    0   11    8  922    0    2    0]\n",
      " [   1   16   19    2    3    1    1  956    1   28]\n",
      " [   6    2    6   10    8   13    6    5  907   11]\n",
      " [   6    7    1   16   26    3    2   14    9  925]]\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('One versus All randomized feature classifier performance: \\n')\n",
    "for i in range(len(f_ovall)):\n",
    "    print(f'\\nWith g_{i+1}(x):\\n')\n",
    "    print('\\tOn training data:\\n')\n",
    "    evaluate_with_feature(f_ovall[i], g[i], Wb, trainX, trainY, K)\n",
    "    print('\\n\\tOn test data:\\n')\n",
    "    evaluate_with_feature(f_ovall[i], g[i], Wb, testX, testY, K)\n",
    "    print('\\n---------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One versus One randomized feature classifier performance: \n",
      "\n",
      "\n",
      "With g_1(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.06213333333333333\n",
      "Confusion matrix: \n",
      " [[5805    3   15    8   11   20   22    6   32    1]\n",
      " [   2 6623   36   17    7   16    2   11   21    7]\n",
      " [  51   69 5521   49   57   20   41   44   93   13]\n",
      " [  26   42  120 5580    8  160   19   48   89   39]\n",
      " [  14   18   20    5 5586   11   14   16    8  150]\n",
      " [  43   46   39  138   22 4970   93   10   47   13]\n",
      " [  27   16   35    2   30   84 5693    0   30    1]\n",
      " [   8   76   54    7   68   10    0 5883    5  154]\n",
      " [  35  192   43  108   48  143   38   25 5154   65]\n",
      " [  21   14   18   83  155   31    3  137   30 5457]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.0696\n",
      "Confusion matrix: \n",
      " [[ 962    0    1    1    0    6    7    3    0    0]\n",
      " [   0 1120    3    3    1    1    4    1    2    0]\n",
      " [   9   17  938   12   10    5   10    9   22    0]\n",
      " [   8    1   19  927    2   19    1    7   21    5]\n",
      " [   3    2    8    2  929    1    7    4    3   23]\n",
      " [   6    5    3   30    8  801   17    2   15    5]\n",
      " [   6    5   12    0    5   19  908    1    2    0]\n",
      " [   1   15   17    3   10    1    0  957    1   23]\n",
      " [   7   17    8   22    9   37   10   10  841   13]\n",
      " [   6    5    1   11   28   12    0   22    3  921]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_2(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.029916666666666668\n",
      "Confusion matrix: \n",
      " [[5867    1    5    3    6    3   14    4   17    3]\n",
      " [   2 6668   27    6    7    2    1    6   15    8]\n",
      " [  20   10 5766   31   29    8   18   29   42    5]\n",
      " [  12    5   75 5845    2   70    4   29   67   22]\n",
      " [  10   11   18    2 5692    1   12    9    5   82]\n",
      " [  28    7   15   73   12 5213   38    3   20   12]\n",
      " [  17    4   15    2    6   42 5819    0   13    0]\n",
      " [   5   20   45    4   30    1    2 6085    5   68]\n",
      " [  11   19   38   68   16   56   16   11 5594   22]\n",
      " [  16   11    7   48   85   21    1   74   30 5656]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.0441\n",
      "Confusion matrix: \n",
      " [[ 967    1    2    0    0    5    3    2    0    0]\n",
      " [   0 1126    3    2    0    0    1    1    2    0]\n",
      " [  10    1  974    5    7    4    8    9   14    0]\n",
      " [   1    1   15  960    0   10    0    5   11    7]\n",
      " [   1    0   10    0  946    1    5    3    2   14]\n",
      " [   8    0    2   24    4  829    9    2   11    3]\n",
      " [  12    3    2    0    3    5  931    0    2    0]\n",
      " [   3    9   22    4    7    0    0  967    1   15]\n",
      " [   6    0    6   12    6   12    4    7  916    5]\n",
      " [   4    5    2    6   22   10    0    9    8  943]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_3(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.7874833333333333\n",
      "Confusion matrix: \n",
      " [[1071 1180  485  456  540  453  416  523  418  381]\n",
      " [ 366 4034  334  317  321  265  292  305  273  235]\n",
      " [ 505 1143 1072  496  519  475  429  499  425  395]\n",
      " [ 567 1224  570  976  484  476  442  485  499  408]\n",
      " [ 536 1116  508  494 1019  411  444  482  436  396]\n",
      " [ 495 1106  499  425  438  873  389  410  415  371]\n",
      " [ 516 1179  518  468  490  455  900  508  488  396]\n",
      " [ 564 1209  562  484  526  484  461 1071  477  427]\n",
      " [ 499 1196  519  467  530  400  440  509  910  381]\n",
      " [ 562 1194  502  480  541  418  465  491  471  825]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.8563\n",
      "Confusion matrix: \n",
      " [[103 196  94  89 105  63  88  93  71  78]\n",
      " [ 75 620  56  58  65  47  54  59  44  57]\n",
      " [ 98 220  94  89  91  82  97  91  89  81]\n",
      " [102 198  97 107  90  68  92  92  80  84]\n",
      " [ 93 192 110  63  96  96  83  78  84  87]\n",
      " [ 90 181  80  71  98  76  68  84  78  66]\n",
      " [ 96 207  86 106  92  79  94  79  66  53]\n",
      " [124 223  90  79 116  90  77  91  66  72]\n",
      " [117 193  88  70  99  79  79  80  85  84]\n",
      " [106 203 100  99  98  80  74  93  85  71]]\n",
      "\n",
      "---------------------------------\n",
      "\n",
      "With g_4(x):\n",
      "\n",
      "\tOn training data:\n",
      "\n",
      "Error rate:  0.02205\n",
      "Confusion matrix: \n",
      " [[5872    1    7    2    7    4   13    2   14    1]\n",
      " [   2 6687   19    5    6    1    1   10    6    5]\n",
      " [  17    6 5816   20   24    3    9   32   27    4]\n",
      " [   7    4   60 5916    2   47    3   21   55   16]\n",
      " [   6    7    6    3 5740    0    8   11    4   57]\n",
      " [  15    2    7   35    6 5291   33    4   17   11]\n",
      " [  14    3    7    0   11   26 5850    0    7    0]\n",
      " [   5   25   30    5   28    3    1 6103    5   60]\n",
      " [   8   16   28   37   11   34   17    9 5675   16]\n",
      " [  12    6    5   40   70   14    1   52   22 5727]]\n",
      "\n",
      "\tOn test data:\n",
      "\n",
      "Error rate:  0.0349\n",
      "Confusion matrix: \n",
      " [[ 969    0    2    0    1    1    3    2    2    0]\n",
      " [   0 1123    2    2    1    1    3    0    3    0]\n",
      " [   6    0  993    6    3    2    3    8   10    1]\n",
      " [   2    0    7  967    0   15    0    6    8    5]\n",
      " [   2    1    7    0  952    0    3    0    2   15]\n",
      " [   6    1    2   17    1  851    6    2    6    0]\n",
      " [   8    2    2    0    3    7  933    0    3    0]\n",
      " [   0    6   18    2    4    1    0  980    4   13]\n",
      " [   2    1    5    6    5    6    6    9  928    6]\n",
      " [   4    5    1   12   17    3    0    6    6  955]]\n",
      "\n",
      "---------------------------------\n"
     ]
    }
   ],
   "source": [
    "print('One versus One randomized feature classifier performance: \\n')\n",
    "for i in range(len(f_ovall)):\n",
    "    print(f'\\nWith g_{i+1}(x):\\n')\n",
    "    print('\\tOn training data:\\n')\n",
    "    evaluate_with_feature(f_ovone[i], g[i], Wb, trainX, trainY, K)\n",
    "    print('\\n\\tOn test data:\\n')\n",
    "    evaluate_with_feature(f_ovone[i], g[i], Wb, testX, testY, K)\n",
    "    print('\\n---------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty much every (model, nonlinear-function) combination seems to keep on generalizing well on test data. Out of the 2, One versus one classifiers tend to do a little bit poorly on test data compared to training data, but just a minimal difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (d) Feature number variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will continue this analysis only considering the best and fastest to compute non-linear function: ReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ls = [800, 900, 1000, 1100, 1200]\n",
    "Ws = []\n",
    "for l in Ls:\n",
    "    Ws.append(np.random.normal(0, 1, size=(l, 784+1)))\n",
    "f_ovall_new = []\n",
    "f_ovone_new = []\n",
    "for w in Ws:\n",
    "    f_ovall_new.append(randomized_features_classifier(trainX, trainY, w, g[3], 'one_vs_all', K))\n",
    "    f_ovone_new.append(randomized_features_classifier(trainX, trainY, w, g[3], 'one_vs_one', K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, m = testX.shape\n",
    "testX_one = ones_padding(testX)\n",
    "\n",
    "h_testX = []\n",
    "for w in Ws:\n",
    "    h_testX.append(g[3](testX_one @ w.T))\n",
    "\n",
    "def error_rate(f_hat, data, labels):\n",
    "    results = np.apply_along_axis(f_hat, 1, data)\n",
    "    new_labels = np.squeeze(labels)\n",
    "    return np.sum(results != new_labels)/data.shape[0]\n",
    "\n",
    "errors = np.zeros((len(Ws), 2))\n",
    "for i in range(len(Ws)):\n",
    "    for j in range(2):\n",
    "        errors[i][0] = error_rate(f_ovall_new[i], h_testX[i], testY)\n",
    "        errors[i][1] = error_rate(f_ovone_new[i], h_testX[i], testY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(Ls, errors[:, 0], 'r')\n",
    "plt.plot(Ls, errors[:, 1], 'g')\n",
    "plt.legend(['One vs All', 'One vs One'])\n",
    "\n",
    "plt.title('Error rate across L')\n",
    "plt.xlabel('L')\n",
    "plt.ylabel('Error rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From what it can be easily seen from the plot, adding more features tends to decrease the error rate of the classifier"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
